[LLM]
PROVIDER = ollama
MODEL = gemma3:4b-it-qat

[ollama]
BASE_URL = http://localhost:11434

[embedding]
MODEL = intfloat/multilingual-e5-small
[vectorstore]
KNOWLEDGE_DIRECTORY = ./vectorstore_rag_plus_knowledge
APPLICATION_DIRECTORY = ./vectorstore_rag_plus_application

[pdf]
PATH = pdfs/

# Optional component-specific configurations
[retrieval]
K = 5

[reranking]
METHOD = llm
TOP_K = 5 # Number of documents after reranking

[debate]
NUM_AGENTS = 4
MAX_ROUNDS = 2

[query_decomposition]
ENABLE_DYNAMIC_DECOMPOSITION = True

[synthesis]
max_tokens = 512
temperature = 0.7

# Example Usage
# To use this, create two directories:
# - vectorstore_rag_plus_knowledge (for knowledge corpus)
# - vectorstore_rag_plus_application (for application corpus)
# Place your PDF documents in the 'pdfs/' directory.
